{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance, Covariance and Correlation\n",
    "\n",
    "## Introduction \n",
    "\n",
    "In our previous lessons, we learned how to calculate the mean and standard deviation. In this lesson, we shall look at how the **Variance** of a random variable is used to calculate the **Covariance** and **Correlation**. These are two key measures used in Statistics for finding the relationship between two random variables. These measures help us determine the degree to which two variables move together. \n",
    "\n",
    "Based on these measures, we can identify if two variables are dependent on each other, and to what extent. This lesson will help you develop a conceptual understanding, perform the necessary calculations and know some precautions to take while using these measures. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Understand and explain data variance and how it relates to standard deviation\n",
    "* Understand and calculate the Covariance and Correlation between two random variables\n",
    "* Visualize and interpret the results of Covariance and Correlation\n",
    "\n",
    "## What is Variance ($\\sigma^2$)\n",
    "\n",
    "Before we talk about covariance, it is imperative that we get some idea about the **Variance** of a random variable. Variance refers to the __spread of a data set__. \n",
    "\n",
    "> __Variance is a measure used to quantify how much a random variable deviates from its mean value__. \n",
    "\n",
    "Let's think about it this way, when calculating variance we are essentially asking; \"__Given the relationship of all given data points, how much distant from mean do we expect the next data point to be?__\"  This \"distance\" is called the **error term**, and it's what variance is measuring. The smaller the variance, the more data points are closer together while the larger the variance, the more spread in the dataset.\n",
    "\n",
    "Variance is shown using notation $\\sigma^2$. Previously, we have seen $\\sigma$ as a measure of standard deviation within a given dataset. Remember, standard deviation is also a measure of spread of data. __Variance is simply the square of standard deviation (Or we could say standard deviation is the square root of variance)__. \n",
    "\n",
    "### Example Use Case\n",
    "\n",
    "Let's consider an example. A simple application of this measure could be associating probabilities with predicted future events in a market research activity and identifying them as \"very likely\" or \"unlikely\" etc. Most people are risk averse, in that, they wish to minimize the amount of risk they must endure to earn a certain level of expected return. \n",
    "\n",
    "Consider the following graphs for Conglomo, Inc. and Bilco, Inc. These graphs show the theoretical frequency distributions of the monthly returns for each firm's common stock as though the returns were normally distributed.\n",
    "\n",
    "<img src=\"images/var.png\" width=400>\n",
    "\n",
    "Conglomo's distribution of returns is more concentrated than Bilco's, as illustrated by Conglomo's relatively narrower bell curve. A more concentrated distribution is defined as having a smaller standard deviation. The distribution curve appears higher, steeper, and narrower because more observations are occurring close to the expected return. Bilco's distribution is rather flat, reflecting that its returns are less concentrated, or more dispersed, than those of Conglomo Inc.\n",
    "\n",
    "If investors were indifferent to risk, they would not be influenced by the differences between stock A and stock B above. Ultimately, the risk-averse investor would clearly prefer stock A. Therefore, most people want to know the range or dispersion(spread/deviation as we termed it earlier) of possible outcomes, as well as the likelihood of certain outcomes occurring. Calculating variance is a great way to find all of the possible values and likelihoods that a random variable can take within a given range defined by the underlying data. \n",
    "\n",
    "### Interpreting Variance \n",
    "\n",
    "A variance value of zero means that all of the values within a data set are identical. All variances that are not equal to zero will be in the form of positive numbers. The larger the variance, the more spread in the data set. A large variance means that the numbers in a set are far from the mean and each other. A small variance means that the numbers are closer together in value. \n",
    "\n",
    "### How to Calculate Variance? \n",
    "\n",
    "Variance is calculated by:\n",
    "1. Taking the differences between each element in a data set and the mean, \n",
    "2. Squaring those differences to give it a positive value\n",
    "3. Dividing the sum of the resulting squares by the number of values in the set.\n",
    "\n",
    "$$\\sigma^2 = \\frac{\\sum(x_i-\\mu)^2}{n}$$\n",
    "\n",
    "Here, $x_i$ represents an individual data point and $\\mu$ represents the mean of the data points. $n$ is the total number of data points. Remember that when calculating a sample variance in order to estimate a population variance, the denominator of the variance equation becomes n - 1. This removes bias from the estimation, as it prohibits the researcher from underestimating the population variance.\n",
    "\n",
    "Following illustration summarizes how spread of data around mean (10) relates to the variance. \n",
    "\n",
    "<img src=\"images/var2.png\" width=500>\n",
    "\n",
    "\n",
    "## Covariance ($\\sigma_{xy}$)\n",
    "\n",
    "Now that we know what variance is and what quantity it measures, imagine calculating variance of two random variables. This gives some idea on how they change together (or stay the same) considering all included values.\n",
    "\n",
    "In Statistics, if we are trying to figure out how two random variables tend to **vary** together, we are effectively talking about the **Covariance** between these variables. Covariance provides an insight into how two variables are __related__ to one another. \n",
    "\n",
    "More precisely, covariance refers to:\n",
    "> The measure of how two random variables in a data set will __change together__. \n",
    "  \n",
    "### How to calculate Covariance ?\n",
    "In essence, covariance is used to measure **how much variables change *together* **, and it is calculated using the formula:\n",
    "\n",
    "\n",
    "$$ \\large \\sigma_{XY} = \\frac{\\sum_{i=1}^{n}(x_i -\\mu_x)(y_i - \\mu_y)}{n}$$\n",
    "\n",
    "Here $X$ and $Y$ are two random variables having n elements each. We want to caluclate ___how much $Y$ depends on $X$___ (or vice-versa), by measuring how values in $Y$ change with observed changes in $X$ values. \n",
    "\n",
    "> This makes $X$ our __independent variable__ and $Y$, the __dependent variable__.  \n",
    "\n",
    "$x_i$ = $i^{th}$ element of variable $X$\n",
    "\n",
    "$y_i$ = $i^{th}$ element of variable $Y$\n",
    "\n",
    "$n$ = number of data points (__$n$ must be same for $X$ and $Y$__)\n",
    "\n",
    "$\\mu_x$ = mean of the independent variable $X$\n",
    "\n",
    "$\\mu_y$ = mean of the dependent variable $Y$\n",
    "\n",
    "$\\sigma_{XY}$ = Covariance between $X$ and $Y$\n",
    "\n",
    "*We can see that above formula calculates the variance of $X$ and $Y$ (check the variance formula above) by multiplying the variance of each of their corresponding elements. Hence the term __Co-Variance__.*\n",
    "\n",
    "### Interpreting Covariance values \n",
    "\n",
    "* A positive covariance indicates that **higher than average** values of one variable correspond with **higher than average** values of the other variable. The same case holds for lower than average values. That is, when **lower than average** values of one variable correspond with **lower than average** values of the other variable, then we have a positive covariance. \n",
    "\n",
    "* Negative covariance indicates that **higher than average** values of one variable correspond with **lower than average** values of the other variable. That is, the variables show opposite behavior.\n",
    "\n",
    "* a zero value, or values close to zero indicate no covariance, i.e. no values from one variable correspond with values of the second variable. \n",
    "\n",
    "This behavior can be further explained using the scatter plots below\n",
    "<img src=\"images/covariance.gif\" width=500> \n",
    "\n",
    "\n",
    "\n",
    "A large negative covariance value shows an inverse relationship between values at x and y axes. i.e. y decreases as x increases. This is shown by the scatter plot on the left. \n",
    "\n",
    "The middle scatter plot shows values spread all over the plot, reflecting the fact that variables on x and y axes cannot be related in terms of how they vary together. The covariance value for such variables would be very close to zero. \n",
    "\n",
    "In the scatter plot on right, we see a strong relationship between values at x and y axes i.e. y increases as x increases. This indicates a large positive covariance\n",
    "\n",
    ">__Covariance is not standardized. Therefore, covariance values can range from negative infinity to positive infinity.__\n",
    "\n",
    "## Correlation \n",
    "\n",
    "So far, we have seen how covariance can identify the degree to which two random variables tend to vary together while using a formulation that depends on the units of the $X$ and $Y$ variables. In data analysis, covariance measure cannot be directly used in data comparison. This is because different experiments may contain underlying data measured in different units. \n",
    "\n",
    "In this case, we need to normalize this degree of variation into a standard unit with interpretable results independent of the units of the data. We achieve this with a derived normalized measure called correlation. \n",
    "\n",
    "Correlation is defined as covariance normalized by the product of the standard deviations of $X$ and $Y$. This normalization helps us set the scale from -1 to 1. So the correlation between ð‘‹ and ð‘Œ would be calculated as:\n",
    "\n",
    "$$Correlation(X,Y) = \\frac{\\sigma_{X,Y}}{\\sigma_X\\sigma_Y}$$\n",
    "\n",
    ">When two random variables **Correlate**, it means that the change in one item **effects** the change in the values of second variable. \n",
    "\n",
    "In data science, the standard practice is typically to look at correlation rather than covariance. This is because correlation is more interpretable since it does not depend on the scale of either random variable involved.\n",
    "\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "\n",
    "#### Social Media and Websites\n",
    "Digital publishers want to maximize their understanding of the potential relationship between social media activity and visits to their website. For example, the digital publisher runs the correlation report between hourly Twitter mentions and visits for a two week period. The correlation coefficient, r, (which we will talk about soon) is found to be r = 0.28, which indicates a medium, positive relationship between Twitter mentions and website visits.\n",
    "\n",
    "#### Optimization for E-retailers\n",
    "E-retailers are interested in driving increased revenue. For example, an e-retailer wants to determine the relationship between a number of secondary success events (e.g. file downloads, product detail page views, internal search click-throughs, etc.) and weekly web revenue. They can quickly identify internal search click-throughs as having the highest correlation; which may indicate an area for optimization.\n",
    "\n",
    "### Types of Correlation Measures\n",
    "\n",
    "__Coefficient of correlation__, r, called the linear correlation coefficient, measures the strength and the direction of a linear relationship between two variables. It is also called __Pearson correlation coefficient__. \n",
    "\n",
    "In Statistics, we measure four types of correlations for detailed relationship analysis: \n",
    "* Pearson correlation \n",
    "* Kendall Rank correlation \n",
    "* Spearman correlation\n",
    "* Point-Biserial correlation. \n",
    "\n",
    "\n",
    "For now, we shall focus on Pearson correlation as it is the go-to correlation measure for most needs. \n",
    "\n",
    "Pearson __r__ correlation is the most widely used correlation statistic to measure the strength of the relationship between two linearly related variables. For the Pearson r correlation, both variables should be normally distributed (normally distributed variables have a bell-shaped curve). Other assumptions include linearity and homoscedasticity. Linearity assumes a straight line relationship between the two variables and homoscedasticity assumes that data is equally distributed about the regression line.\n",
    "\n",
    "\n",
    "### Calculating Coefficient of Correlation (r)\n",
    "\n",
    "Pearson Correlation (r) is calculated using following formula :\n",
    "\n",
    "$$ r = \\frac{\\sum_{i=1}^{n}(x_i -\\mu_x)(y_i - \\mu_y)} {\\sqrt{\\sum_{i=1}^{n}(x_i - \\mu_x)^2 \\sum_{i=1}^{n}(y_i-\\mu_y)^2}}$$\n",
    "\n",
    "So just like in the case of covariance,  $X$ and $Y$ are two random variables having n elements each. \n",
    "\n",
    "\n",
    "$x_i$ = $i^{th}$ element of variable $X$\n",
    "\n",
    "$y_i$ = $i^{th}$ element of variable $Y$\n",
    "\n",
    "$n$ = number of data points (__$n$ must be same for $X$ and $Y$__)\n",
    "\n",
    "$\\mu_x$ = mean of the independent variable $X$\n",
    "\n",
    "$\\mu_y$ = mean of the dependent variable $Y$\n",
    "\n",
    "$r$ = Calculated Pearson Correlation\n",
    "\n",
    "\n",
    "Here x and y are the random variables, x_bar and y_bar are the mean values for both x and y. A detailed mathematical insight into this equation is available [in this paper](http://www.hep.ph.ic.ac.uk/~hallg/UG_2015/Pearsons.pdf)\n",
    "\n",
    "### Interpreting Correlation values\n",
    "\n",
    "> __The Correlation formula shown above always gives values in a range between -1 and 1__\n",
    "\n",
    "If two variables have a correlation of +0.9,  this means the change in one variable results in an almost similar change to the other variable. A correlation value of -0.9 means that the change is one variable results as an opposite change in the other variable. A pearson correlation near 0 would mean the variables have no effect on each other. Here are some examples of pearson correlation calculations as scatter plots. \n",
    "![](pearson_2.png)\n",
    "\n",
    "Let's think about stock markets in terms of correlation. All the stock market indexes tend to move together in similar directions. When the DOW Jones loses 5%, the S&P 500 usually loses around 5%. When the DOW Jones gains 5%, the S&P 500 usually gains around 5% because they are **highly positively correlated**.\n",
    "\n",
    "On the other hand, there could also be a negative correlation where you might observe that as the DOW Jones loses 5% of its value, Gold might gain 5%. Alternatively, if the Dow Jones gains 5% of its value, Gold may lose 5% of its value. That's **negative correlation**. \n",
    "\n",
    "### So how do these measures relate to each other ?\n",
    "\n",
    "Are Covariance and Correlation The Same Thing? Simply put, no.\n",
    "\n",
    "While both covariance and correlation indicate whether variables are positively or inversely related to each other, they are not considered to be the same. This is because correlation also informs about the degree to which the variables tend to move together. \n",
    "\n",
    "Covariance is used to measure variables that have different units of measurement. By leveraging covariance, analysts are able to determine whether units are increasing or decreasing. However, they are unable to solidify the degree to which the variables are moving together due to the fact that covariance does not use one standardized unit of measurement.\n",
    "\n",
    "Correlation, on the other hand, standardizes the measure of interdependence between two variables and informs researchers as to how closely the two variables move together.\n",
    "\n",
    "## Summary\n",
    "In this lesson, we first saw that the variance of random variables is a measure of deviation from the mean. We also saw how this measure can be used to first calculate covariance, followed by the correlation to analyze how a change in one variable effects the change of another variable. \n",
    "\n",
    "In our next lesson, we shall see how we can use correlation analysis to run a __regression analysis__ and later, how covariance calculation helps us with dimensionality reduction. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
